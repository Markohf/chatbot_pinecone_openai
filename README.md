"""
# ğŸ’¬ Chatbot de Conocimiento con RAG + Pinecone + OpenAI

Este proyecto implementa un **chatbot de recuperaciÃ³n aumentada (RAG)** que permite hacer preguntas sobre documentos internos cargados localmente.  
El sistema combina **LangChain**, **Pinecone** y **OpenAI** para indexar documentos, generar embeddings y responder consultas con base en esos datos.

---

## ğŸ§© Arquitectura general

El flujo completo del proyecto se divide en dos partes:

### 1. **ConstrucciÃ³n de la base de conocimiento (`build_knowledge_base.py`)**
- Lee los archivos almacenados en `data_rag/`.
- Los divide en fragmentos (`chunks`) de texto.
- Genera embeddings usando un modelo HuggingFace.
- Los sube a **Pinecone**, creando o actualizando el Ã­ndice correspondiente.

### 2. **AplicaciÃ³n interactiva (`app.py`)**
- Desarrollada con **Streamlit**.
- Permite al usuario hacer preguntas en espaÃ±ol.
- Recupera los fragmentos mÃ¡s relevantes desde Pinecone.
- EnvÃ­a la pregunta + contexto al modelo de OpenAI configurado.
- Muestra Ãºnicamente la respuesta final, sin fuentes ni detalles tÃ©cnicos.

---

## ğŸ“‚ Estructura del proyecto
```
despliegue_exam/
â”‚
â”œâ”€â”€ .streamlit/
â”‚ â””â”€â”€ secrets.toml # Variables de entorno y claves de API
â”‚
â”œâ”€â”€ data_rag/ # Carpeta con los documentos fuente
â”‚ â”œâ”€â”€ ejemplo1.pdf
â”‚ â”œâ”€â”€ ejemplo2.txt
â”‚ â””â”€â”€ ...
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ __init__.py
â”‚ â”œâ”€â”€ app.py # AplicaciÃ³n principal (Streamlit)
â”‚ â”œâ”€â”€ build_knowledge_base.py # Script para indexar documentos en Pinecone
â”‚ â””â”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ logger_config.py # ConfiguraciÃ³n de logging
â”‚
â”œâ”€â”€ requirements.txt # LibrerÃ­as necesarias
â””â”€â”€ README.md # Este archivo
```

---

## âš™ï¸ ConfiguraciÃ³n inicial

### 1. Crear y activar entorno virtual

Desde la raÃ­z del proyecto:

#### En Windows (PowerShell):
```
python -m venv venv
venv\Scripts\Activate.ps1
```

#### En Linux / macOS / Git Bash:
```
python -m venv venv
source venv/bin/activate
```

### 2. Instalar dependencias

Con el entorno activado:
```
pip install -r requirements.txt
```

### 3. Configurar variables en .streamlit/secrets.toml

Crea el archivo `.streamlit/secrets.toml` (si no existe) en la raÃ­z del proyecto con el siguiente formato:
```
OPENAI_API_KEY = ""
PINECONE_API_KEY = ""

INDEX_NAME = ""
NAMESPACE = ""
EMBEDDING_MODEL = ""

PINECONE_CLOUD = ""
PINECONE_REGION = ""

OPENAI_MODEL = ""
```

âš ï¸ Reemplaza los valores vacÃ­os por tus credenciales y parÃ¡metros reales.

- INDEX_NAME: nombre del Ã­ndice que se crearÃ¡ en Pinecone.
- NAMESPACE: etiqueta o agrupador dentro del Ã­ndice.
- EMBEDDING_MODEL: nombre del modelo HuggingFace usado para generar embeddings (por ejemplo "sentence-transformers/all-MiniLM-L6-v2").
- PINECONE_CLOUD / REGION: configuraciÃ³n del entorno donde se crea el Ã­ndice (por ejemplo "aws" y "us-east-1").
- OPENAI_MODEL: modelo de chat usado para responder (por ejemplo "gpt-4o-mini").

---

## Cargar y actualizar documentos

Coloca tus archivos de referencia dentro de la carpeta data_rag/.

Formatos soportados:
- .pdf
- .txt
- .md
- .csv

Cada vez que agregues o actualices documentos, reconstruye la base de conocimiento con:
```
python -m src.build_knowledge_base
```
El script:
- Lee los documentos nuevos.
- Los divide en fragmentos.
- Calcula embeddings.
- Los envÃ­a a Pinecone (creando el Ã­ndice si no existe).

---

## ğŸš€ Ejecutar la aplicaciÃ³n

Desde la raÃ­z del proyecto:
```
streamlit run src/app.py
```
Luego abre el enlace local que Streamlit mostrarÃ¡, por ejemplo:
```
# Ejemplo
http://localhost:9999
```

---

## ğŸ§± Detalles tÃ©cnicos

### build_knowledge_base.py
- Usa pypdf para extraer texto de archivos PDF.
- Implementa RecursiveCharacterTextSplitter (de LangChain) para generar fragmentos superpuestos.
- Crea embeddings mediante HuggingFaceEmbeddings.
- Verifica si el Ã­ndice existe en Pinecone y, si no, lo crea automÃ¡ticamente usando ServerlessSpec.
- Subida de documentos a Pinecone con IDs deterministas (hash SHA-256) para evitar duplicados.

### app.py
- Desarrollado con Streamlit para interfaz web.
- Conecta a Pinecone y reconstruye el retriever dinÃ¡micamente.
- Implementa un pipeline RetrievalQA (de LangChain) con un prompt personalizado en espaÃ±ol.
- Procesa preguntas con solo presionar Enter (sin botÃ³n manual).
- Muestra Ãºnicamente la respuesta, sin detalles ni fuentes.

---

## ğŸ§© Dependencias principales

Las versiones se encuentran en requirements.txt.
Incluye entre otras:
```
streamlit
langchain
langchain-community
langchain-openai
langchain-pinecone
pinecone
sentence-transformers
torch
huggingface-hub
pypdf
toml
python-dotenv
httpx
```

---

## ğŸ§ª EjecuciÃ³n tÃ­pica

### Construir base de conocimiento:
```
python -m src.build_knowledge_base
```

### Ejecutar la app:
```
streamlit run src/app.py
```